----------------------------------------------------------------------------------------------------------------------------------

Lab1.1

sudo ls 查看执行权限
如果 sudo ls不可用需要执行以下操作，例如用户为student
    su
    cd /etc/sudoers.d
    cat /etc/sudoers.d/student
-----------------------------------
    student ALL=(ALL) ALL
-----------------------------------
    chmod 440 /etc/sudoers.d/student

建议在.bashrc中添加PATH=$PATH:/usr/sbin:/sbin

----------------------------------------------------------------------------------------------------------------------------------

Lab2.1

https://kubernetes.io/
右上角可以切换版本

https://github.com/kubernetes/kubernetes/

----------------------------------------------------------------------------------------------------------------------------------

Lab3.1 安装Kubernetes

课程Yaml文件下载
wget https://training.linuxfoundation.org/cm/LFS258/LFS258_V2018-02-15_SOLUTIONS.tar.bz2 --user=LFtraining --password=Penguin2014
tar -xvf ./LFS258_V2018-02-15_SOLUTIONS.tar.bz2

使用秘钥登录系统
$ ssh -i LFS458.pem student@35.226.100.87

安装Docker
$ sudo -i
# apt-get update && apt-get upgrade -y
# apt-cache madison docker.io   //查看可用的Docker版本
# apt-get install -y docker.io

<国内配置科学上网>
# mkdir -p /etc/systemd/system/docker.service.d
# cat /etc/systemd/system/docker.service.d/http-proxy.conf
[Service]
Environment="HTTP_PROXY=http://192.168.100.97:8123"
# cat /etc/systemd/system/docker.service.d/https-proxy.conf
[Service]
Environment="HTTPS_PROXY=https://192.168.100.97:8123"
# systemctl daemon-reload
# systemctl restart docker
# docker info
# systemctl show --property=Environment docker

关SWAP
# swapoff  -a
# vim /etc/fstab  (add #)

添加Kubernetes源并安装
# vim /etc/apt/sources.list.d/kubernetes.list
deb http://apt.kubernetes.io/ kubernetes-xenial main
# https_proxy=https://192.168.100.97:8123 curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
# https_proxy=https://192.168.100.97:8123 apt-get update
# apt-cache madison kubeadm
# https_proxy=https://192.168.100.97:8123 apt-get install kubeadm=1.9.6-00 kubelet=1.9.6-00 //官方手册是1.9.1-00

kubernetes初使化
# kubeadm init --pod-network-cidr 10.244.0.0/16  --apiserver-advertise-address 192.168.100.65 //cidr是Flannel中的地址段
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Your Kubernetes master has initialized successfully!
To start using your cluster, you need to run the following as a regular user:
  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config
You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
https://kubernetes.io/docs/concepts/cluster-administration/addons/
You can now join any number of machines by running the following on each node
as root:
  kubeadm join --token 50b8ee.cd69db91cbdf3177 192.168.100.65:6443 --discovery-token-ca-cert-hash sha256:b84b4143609d968215d5cb61c69b2b417a407d30c53b70f975dc787b24c56879
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
根据上面的地址 https://kubernetes.io/docs/concepts/cluster-administration/addons/ 打开后选择要配置的网络
本实验使用的网络是Flannel，打开上面的地址后找Flannel，跳到GitHub地址
https://github.com/coreos/flannel/blob/master/Documentation/kubernetes.md
向上翻一级到 Documentation 目录，找到kube-flannel.yml文件并打开，然后点Raw按钮可得到以下地址
https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
# exit

普通用户授权使用Kubectl命令
$ mkdir -p $HOME/.kube
$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
$ sudo chown $(id -u):$(id -g) $HOME/.kube/config

安装Flannel网络
$ sudo cp /root/kube-flannel.yml ./
$ kubectl apply -f kube-flannel.yml
$ ip addr 

去除master节点的Taints的NoSchedule,使master节点可以部署pod
$ kubectl get nodes
$ kubectl get pods --all-namespaces
$ kubectl describe nodes yqb-k8s-node1 | grep Taints
$ kubectl taint nodes --all  node-role.kubernetes.io/master-
$ kubectl describe nodes yqb-k8s-node1 | grep Taints

Kube命令自动补齐
$ source <(kubectl completion bash)
$ echo "source <(kubectl completion bash)" >> .bashrc
$ source <(kubeadm completion bash)
$ echo "source <(kubeadm completion bash)" >> .bashrc
----------------------------------------------------------------------------------------------------------------------------------

Lab3.2 扩展集群

安装WorkNode与Lab3.1中的部分内容一样
    1、安装Docker
    2、<国内配置科学上网>
    3、关SWAP
    4、添加Kubernetes源并安装
    
生成令牌，令牌默认保存24h
$ sudo kubeadm token list
如果没有令牌使用命令创建
$ sudo kubeadm token create
    --ttl 0 //创建永不过期的token

证书-->生成公钥-->签名
$ openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outfrom der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'
e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855

    openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt //-pubkey使用证书文件生成公钥
    open rsa -pubin -outfrom der 2>/dev/null //-pubin输出公钥，-outfroml输出格式，标准错误到null
    openssl dgst -sha256 -hex | sed 's/^.* //' //使用sha256生成和验证数字签名
    
将主机加入到集群，使用已有token或新创建的token,并把上面的加密内容放到sha256:后面
$ kubeadm join --token 50b8ee.cd69db91cbdf3177 192.168.100.65:6443 --discovery-token-ca-cert-hash sha256:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855

$ kubectl get nodes
$ kubectl get namespace
$ kubectl get pod --all-namespaces
$ ip addr //两个节点查看flannel.1的网络分别是10.244.0.0/32和10.244.1.0/32

部署应用测试及网络连通性
$ kubectl run nginx --image nginx
$ kubectl get deployments
$ kubectl describe deployment nginx
$ kubectl get events //查看集群获取和部署新应用程序所需的基本步骤
$ kubectl get deployment ng -o yaml
$ kubectl get deployment ng -o yaml > first.yaml
$ vim first.yaml //编辑并删除 creationTimestamp、resourceVersion、selfLink、uid和status下的十几行
$ kubectl create -f first.yaml
$ kubectl get deployment nginx -o yaml > second.yaml
$ diff first.yaml second.yaml
$ kubectl expose deployment/nginx //因为yaml文件中没有为Pod指定端口，所以暴露服务会报错
$ vim first.yaml //在容器的name下增加这三行
          ports:
          - containerPort: 80
            protocol: TCP
$ kubectl delete deployment nginx
$ kubectl apply -f first.yaml
$ kubectl expose deployment nginx
$ kubectl get service nginx
$ kubectl get endpoints nginx
$ kubectl describe pod nginx-7587c6fdb6-jlg5h  | grep Node //查看容器所在主机
# sudo tcpdump -i flannel.1  //在Pod所在机器抓包，包括flannel.1、cni0和veth网卡
$ curl 10.99.157.33:80 //使用群集IP访问
$ curl 10.244.1.187:80 //使用endpoint地址访问

$ kubectl get deployment nginx
$ kubectl scale deployment nginx --replicas=3 //扩展为3个pod
$ kubectl get deployment nginx
$ kubectl get endpoints nginx
$ kubectl get pod -o wide
$ kubectl delete pod nginx-7587c6fdb6-jlg5h //删除一个pod
$ kubectl get pod //发现自动重建pod,维持为三副本
$ kubectl get endpoints //发现endpoint自动更新的新的Pod地址
$ echo 111111111111111111111111 >> /usr/share/nginx/html/index.html //分别进入三个容器，往index.html文件下面分别加111、222、333
$ curl 10.99.157.33:80 //执行多次可以看到自动负载均衡

----------------------------------------------------------------------------------------------------------------------------------

Lab3.3 从集群外部访问

$ kubectl get pod
$ kubectl exec nginx-7587c6fdb6-bxj9s -- printenv | grep KUBERNETES //打印容器中的环境变量
$ kubectl get service
$ kubectl delete service nginx //删除服务
$ kubectl expose deployment nginx --type=LoadBalancer //创建一个类型为负载均衡的服务,如果类型为NodePort也可用以下方法正常访问
$ kubectl get service //查看暴露的端口为80:30063/TCP AWS和GCE有一个奇怪的地方，EXTERNAL-IP它总是显示为 <pending> 
使用浏览器访问 http://42.62.51.27:30063/
$ kubectl delete deployment nginx
$ kubectl delete endpoints nginx
$ kubectl delete service nginx

----------------------------------------------------------------------------------------------------------------------------------

Lab4.1 容器CPU和内存限制

$ kubectl run hog --image=vish/stress    //virsh/stress是压力容器
$ kubectl get deployment
$ kubectl describe deployment hog
$ kubectl get deployment hog -o yaml    //没有设置资源限制，resources: {}里是空的

$ kubectl get deployment hog -o yaml > hog.yaml
$ vim hog.yaml //删除状态等内容
        resources:   //删除{}添加下面四行
          limits:
            memory: "4Gi"
          requests:
            memory: "256Mi"
$ kubectl replace -f hog.yaml    //再次发布
$ kubectl get deployment hog -o yaml
$ kubectl run hog --image=vish/stress --limits='memory=4Gi' --requests='memory=2500Mi' //也可以用这个命令直接做资源限制
$ kubectl logs hog-ddcc66b88-wfm2q    //查看容器资源限制为0

$ vim hog.yaml //修改配置文件
        resources:
          limits:
            cpu: 1
            memory: 4Gi
          requests:
            cpu: 0.5
            memory: 500Mi
        args:    //给容器增加压力测试参数，每次分配100M间隔1s总计950M
        - -cpus
        - "2"
        - -mem-total
        - "950Mi"
        - -mem-alloc-size
        - "100Mi"
        - -mem-alloc-sleep
        - "1s"
$ kubectl replace -f hog.yaml    //再次发布
$ kubectl logs hog-67594d8556-48w65    //查看资源消耗950M内存和2个CPU线程，可以在节点上查看内存的使用

资源请求（Request）：容器所需的最小资源需求
和资源限制（Limit）：是容器不能超过的资源上限

----------------------------------------------------------------------------------------------------------------------------------

Lab 4.2 NameSpace资源限制

$ kubectl create namespace low-usage-limit
$ kubectl get namespace
$ vim low-resource-range.yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: low-resource-range
spec:
  limits:
  - default:
      cpu: 1
      memory: 1000Mi
    defaultRequest:
      cpu: 0.2
      memory: 100Mi
    type: Container
$ kubectl apply -f low-resource-range.yaml --namespace=low-usage-limit
$ kubectl get LimitRange --all-namespaces
$ kubectl run limited-hog --image=vish/stress -n low-usage-limit
$ kubectl get deployment --all-namespaces
$ kubectl get pod -n low-usage-limit
$ kubectl -n low-usage-limit get pod limited-hog-5bcbbc876b-hjfrw -o yaml    //查看Pod详情，容器资源限制将从Namespace继承

在LimitRange中，容器默认使用0.2C和100M内存，最多限制为1C和1000M内存，当容为20个的时候，有14个Running，6个Pending
此时14个容器占用总CPU和内存数达到了namespace中LimitRange的上限1C、1000M
修改了low-resource-range.yaml配置文件后，重新replace配置后，对已经存存的容器无限效，通过scale调度新增加的容器使用新的LimitRange

$ cp hog.yaml hog2.yaml
$ vim hog2.yaml
    namespace: low-usage-limit    //改默认namespace
$ kubectl apply -f hog2.yaml
查看top 

----------------------------------------------------------------------------------------------------------------------------------

Lab 4.3 更复杂的部署

一个网站的微服务部署Demo，github地址是
https://raw.githubusercontent.com/microservices-demo/microservices-demo/master/deploy/kubernetes/complete-demo.yaml

$ http_proxy=http://192.168.100.97:8123/ wget https://tinyurl.com/y8bn2awp -O complete-demo.yaml
$ cat complete-demo.yaml | grep image    //查看所需要的镜像
$ vim complete-demo.yaml    //详细查看文件，并找到namespace
$ kubectl create namespace sock-shop
$ kubectl apply -f complete-demo.yaml -n sock-shop
$ kubectl get service -n sock-shop    //查看NodePort并访问网站地址http://42.62.51.27:30001/

测试删除Pod后，Pod会自动重建,删除Deployment和Service后不会自动重建

$ kubectl delete -f complete-demo.yaml

----------------------------------------------------------------------------------------------------------------------------------

Lab 5.1 配置TLS访问

API请求必须以JSON形式传递信息。
在为您做API请求时，kubectl将.yaml转换为JSON。
API请求有很多设置，但必须包括apiVersion、kind和元数据，以及spec设置，以声明要部署的容器类型。
spec字段取决于正在创建的对象。

访问API
$ cat .kube/config | grep client-cert | awk '{print $2}' | base64 -d > client.pem
$ cat .kube/config | grep client-key | awk '{print $2}' | base64 -d > client-key.pem
$ cat .kube/config | grep  auth | awk '{print $2}' | base64 -d  > ca.pem
$ cat .kube/config | grep server

$ curl --cert client.pem --key client-key.pem --cacert ca.pem https://192.168.100.65:6443/api/v1/pods

使用json文件通过调用API创建pod
$ vim curlpod.json
{
  "kind": "Pod",
  "apiVersion": "v1",
  "metadata":{
    "name": "curlpod",
    "namespace": "default",
    "labels": {
      "name": "examplepod"
    }
  },
  "spec": {
    "containers": [{
      "name": "nginx",
      "image": "nginx",
      "ports": [{"containerPort": 80}]
    }]
  }
}

$ curl --cert client.pem --key client-key.pem --cacert ca.pem https://192.168.100.65:6443/api/v1/namespaces/default/pods -XPOST -H 'Content-Type: application/json' -d@curlpod.json
$ kubectl get pod

----------------------------------------------------------------------------------------------------------------------------------

Lab 5.2 配置TLS访问

$ kubectl get endpoints
$ strace kubectl get endpoints    //跟踪调试运行

查看所有的API
cd .kube/cache/discovery/192.168.100.65_6443/
find . | grep json

$ kubectl api-versions

$ python -m json.tool v1/serverresources.json    //以json格式查看API内容
$ python -m json.tool v1/serverresources.json | grep kind
$ python -m json.tool ./apps/v1beta1/serverresources.json | grep kind
$ python -m json.tool ./autoscaling/v1/serverresources.json | grep kind

----------------------------------------------------------------------------------------------------------------------------------

Lab 6.1 RESTful API 访问（有状态集API访问）

我们将使用curl命令以不安全的方式向集群发出API请求。
一旦我们知道了IP地址和端口，我们就可以用RESTful方式检索集群数据。
默认情况下，大多数信息是受限的，但对身份验证策略的更改可能允许更多访问。

$ kubectl config view

接下来，我们需要找到不记名令牌。
这是默认令牌的一部分。
查看令牌列表，首先在群集上，然后是默认名称空间中的令牌。
集群中的每个controllers都有一个seceret

$ kubectl get secret --all-namespaces
$ kubectl get secret 
$ kubectl describe secret default-token-vx5kg
$ kubectl describe secret default-token-vx5kg | grep token: | awk '{print $2}'
$ export token=$(kubectl describe secret default-token-vx5kg | grep token: | awk '{print $2}')

$ curl https://192.168.100.65:6443/apis --header "Authorzation : Bearer $token" -k
$ curl https://192.168.100.65:6443/api/v1 --header "Authorzation : Bearer $token" -k
$ curl https://192.168.100.65:6443/api/v1/namespaces --header "Authorzation : Bearer $token" -k    //报错
$ kubectl run -i -t busybox --image=busybox --restart=Never
# ls /var/run/secrets/kubernetes.io/serviceaccount/

----------------------------------------------------------------------------------------------------------------------------------

Lab 6.2 使用Proxy

$ kubectl proxy --api-prefix=/ &
$ curl http://127.0.0.1:8001/api
$ curl http://127.0.0.1:8001/api/v1/namespaces     //不报错

----------------------------------------------------------------------------------------------------------------------------------

Lab 6.3 使用 CronJob

每分钟创建一个容器，输出date后sleep 30s，然后关闭
$ vim cron-job.yaml

apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: date
spec:
  schedule: "*/1 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: dateperminute
            image: busybox
            args:
            - /bin/sh
            - -c
            - date; sleep 30
          restartPolicy: OnFailure

$ kubectl create -f cron-job.yaml
$ kubectl get cronjob
$ kubectl get job
$ kubectl get pod
$ kubectl get job --watch
$ kubectl get pod | grep date

查看容器日志，每个容器创建时间相差1分钟
$ kubectl logs date-1529483160-s86f9
$ kubectl logs date-1529483220-rtwhs
$ kubectl logs date-1529483280-9674n
$ kubectl logs date-1529483340-l8jz2

$ kubectl delete cronjob date

----------------------------------------------------------------------------------------------------------------------------------

Lab 7.1 使用 ReplicaSets （副本集）

$ kubectl get replicaset
$ vim rs.yaml

apiVersion: extensions/v1beta1
kind: ReplicaSet
metadata:
  name: rs-one
spec:
  replicas: 2
  template:
    metadata:
      labels:
        system: ReplicaOne
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80

$ kubectl create -f rs.yaml
$ kubectl describe replicaset rs-one
$ kubectl get pod
$ kubectl delete replicaset rs-one --cascade=false    //删除Replicaset不串联pod
$ kubectl get replicaset
$ kubectl get pod

$ kubectl create -f rs.yaml    //再次创建rs，rs会根据selector自动关联pod
$ kubectl get replicaset
$ kubectl get pods

$ kubectl edit pod rs-one-smjdj    //修改Pod的Lable，ReplicaOne改成IsolatedPod
$ kubectl get replicaset    //复本数应该还是2
$ kubectl  get pod -L system    //3个pod
$ kubectl delete replicaset rs-one    //rs和2个pod将被删除
$ kubectl delete pod -l system=IsolatedPod    //通过lable删除剩下的pod

可以删除复本集而不删除pod
创建复本集后可自动关联pod
修改pod标签后复本集自动创建新的pod的以维护副本数量
可通过标签删除pod

----------------------------------------------------------------------------------------------------------------------------------

Lab 7.2 使用 DaemonSets （守护程序集）

vim ds.yaml

apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: ds-one
spec:
  template:
    metadata:
      labels:
        system: DaemonSetOne
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80

$ kubectl create -f ds.yaml
$ kubectl get daemonset
$ kubectl describe daemonset ds-one
$ kubectl get pod -o wide
$ kubectl get pod -o wide
$ kubectl describe pod ds-one-7s5xh | grep 

与ReplicaSets相比，只是Yaml文件修改了一下Kind和Labels，删除了 replicas: 2

----------------------------------------------------------------------------------------------------------------------------------

Lab 7.3 滚动更新和回滚

$ kubectl get daemonset ds-one -o yaml    //查看默认更新策略为OnDelete
$ kubectl set image daemonset ds-one nginx=nginx:1.8.1-alpine    //使用set更新镜像
$ kubectl describe pod ds-one-7s5xh | grep Image:    //查看原来的Pod image版本没有变
$ kubectl delete pod ds-one-7s5xh
$ kubectl get pod
$ kubectl describe pod ds-one-qmsvb | grep Image:    //新增pod image版本更新了
$ kubectl rollout history daemonset ds-one    //查看更新历史列表
$ kubectl rollout history daemonset ds-one --revision=1    //对比两个历史版本的区别
$ kubectl rollout history daemonset ds-one --revision=2

$ kubectl rollout undo daemonset ds-one --to-revision=1 //回滚到版本1
$ kubectl describe pod ds-one-qmsvb | grep Image:
$ kubectl delete pod ds-one-qmsvb
$ kubectl describe pod ds-one-x9mtg  | grep Image:    //回到了原来的版本

滚动更新使用set，回滚使用 rollout undo
当updateStrategy为OnDelete时，先删除原有的Pod后，新Pod才会更新

$ kubectl get daemonset ds-one -o yaml    //查看默认更新策略为OnDelete
$ kubectl get daemonset ds-one -o yaml > ds2.yaml
$ vim ds2.yaml    //删除该删的时间和状态，修改OnDelete为RollingUpdate,修改名字为ds-two
$ kubectl create -f ds2.yaml    //重新创建ds-two
$ kubectl get pod
$ kubectl describe pod ds-two-gtv48  | grep Image:
$ kubectl edit daemonset ds-two    //使用edit更新image为nginx:1.8.1-alpine
$ kubectl get daemonset ds-two
$ kubectl get pod
$ kubectl describe pod ds-two-q7jlp | grep Image:
$ kubectl rollout status daemonset ds-two    //查看循环更新状态
$ kubectl rollout history daemonset ds-two
$ kubectl rollout history daemonset ds-two --revision=2
$ kubectl delete daemonset ds-two

当updateStrategy为RollingUpdate时，Pod会自动循环更新（删一个建一个）

----------------------------------------------------------------------------------------------------------------------------------

Lab 8.1 Service

服务（也称为微服务）是声明访问Pod的逻辑集合的策略的对象。

$ vim nginx-one.yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx-one
  labels:
    system: secondOne
  namespace: accounting
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - image: nginx:1.7.9
        imagePullPolicy: Always
        name: nginx
        ports:
        - containerPort: 8080
          protocol: TCP
      nodeSelector:
        system: secondOne

$ kubectl get node --show-labels
$ kubectl create namespace accounting
$ kubectl -n accounting get deployment
$ kubectl -n accounting get pod    //状态是Pending
$ kubectl -n accounting describe pod    //看到报错，没有可用节点

$ kubectl label node yqb-k8s-node2 system=secondOne    //为节点添加Lable后，容器启动成功
$ kubectl get nodes --show-labels
$ kubectl get pods -n accounting

$ kubectl -n accounting get pod --show-labels    //查看容器的Lable
$ kubectl get pod --all-namespaces -l app=nginx    //通过Lable筛选容器

$ kubectl get service -n accounting
$ kubectl -n accounting expose deployment nginx-one    //服务暴露
$ kubectl get service -n accounting
$ kubectl get endpoints -n accounting

虽然暴露的是8080端口，但是应用未侦听此端口，所以访问endpoints的8080端口失败，访问80端口成功

$ kubectl delete deployment nginx-one -n accounting    //删除Deployment后，Service和Endpoints并不会被删除
$ vim nginx-one.yaml    //8080改成80
$ kubectl create -f nginx-one.yaml    //重新部署
kubectl -n accounting delete service nginx-one    //删除Service
kubectl -n accounting expose deployment nginx-one    //服务暴露，默认为ClusterIP
kubectl get endpoints -n accounting
kubectl get service -n accounting

kubectl -n accounting delete service nginx-one    //删除服务暴露，为后面的实验做准备

----------------------------------------------------------------------------------------------------------------------------------

Lab 8.2 NodePort

$ kubectl expose deployment -n accounting nginx-one --type=NodePort --name=service-lab    //接上一章的Deployment，暴露服务为NodePort，启名为service-lab
$ kubectl get service -n accounting
$ kubectl describe service -n accounting    //查看自动生成的端口30567

$ curl 192.168.100.65:30567
http://42.62.51.27:30567/    //使用外部网络，访问这个地址也可以访问到

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Lab 8.3 使用Lables管理资源

$ kubectl delete pod -n accounting -l app=nginx    //通过lable删除pod
$ kubectl get pod -n accounting    //deployment自动部署新的pod
$ kubectl get deployment -n accounting --show-labels    //查看deployment的lable
$ kubectl delete deployment -n accounting -l system=secondOne    //通过lable删除deployment

$ kubectl get node --show-labels
$ kubectl label node yqb-k8s-node2 system-    //从节点上删除lable

----------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------------------------

##学习扩展##

Kubernetes支持暴露端服务的四种类型
    $ kubectl expose deployment nginx --type=xxx
    "ClusterIP", "ExternalName", "LoadBalancer", "NodePort"

ClusterIP：
    默认只允许集群内部访问
    外部访问时可以通过开启Kubernetes proxy 模式，使用API地址来访问服务
        $ kubectl proxy --port=8080
        http://localhost:8080/api/v1/proxy/namespaces/<NAMESPACE>/services/<SERVICE-NAME>:<PORT-NAME>/ 

NodePort：
    自动创建ClusterIP
    在所有节点上开放一个特定端口，任何发送到该端口的流量都被转发到ClusterIP
    端口范围只能是 30000-32767
    如果节点/VM 的 IP 地址发生变化，用户访问时需要输入新的地址


LoadBalancer：
    自动创建ClusterIP
    自动创建NodePort
    依赖于IaaS服务
    IaaS提供一个单独的 IP 地址，转发所有流量到你的服务

ExternalName：
    使用CNAME记录,将服务映射到外部

删除namespace后，namespace下的所有资源将被删除


----------------------------------------------------------------------------------------------------------------------------------

###排错###

环境描述：
node1 双网卡
node2 单网卡

问题描述：
跨节点无法访问Cluster IP
node1 和 node2 之间的10.244.0.0段的地址无法通信

解决过程：
kubectl -n kube-system logs -f kube-flannel-ds-9n9ww kube-flannel   //查看flannel日志

1 main.go:475] Determining IP address of default interface
1 main.go:488] Using interface with name ens3 and address 192.168.200.227
1 main.go:505] Defaulting external address to interface address (192.168.200.227)
1 kube.go:131] Waiting 10m0s for node controller to sync
1 kube.go:294] Starting kube subnet manager
1 kube.go:138] Node controller sync successful
1 main.go:235] Created subnet manager: Kubernetes Subnet Manager - yqb-k8s-node1
1 main.go:238] Installing signal handlers
1 main.go:353] Found network config - Backend type: vxlan

发现选择了ens3为flannel的网卡，应该使用ens4
kubectl delete -f kube-flannel.yml
reboot
vim kube-flannel.yml
        - /opt/bin/flanneld
        - --iface-regex=192.168.100.*    //增加这一行
        - --iface=ens3    //如果每台机器的网卡名都一样，可以使用这一行
kubectl apply -f  kube-flannel.yml

参考：
https://github.com/coreos/flannel/issues/981
https://github.com/coreos/flannel/issues/967

