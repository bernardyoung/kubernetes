----------------------------------------------------------------------------------------------------------------------------------

Lab1.1

sudo ls 查看执行权限
如果 sudo ls不可用需要执行以下操作，例如用户为student
    su
    cd /etc/sudoers.d
    cat /etc/sudoers.d/student
-----------------------------------
    student ALL=(ALL) ALL
-----------------------------------
    chmod 440 /etc/sudoers.d/student

建议在.bashrc中添加PATH=$PATH:/usr/sbin:/sbin

----------------------------------------------------------------------------------------------------------------------------------

Lab2.1

https://kubernetes.io/
右上角可以切换版本

https://github.com/kubernetes/kubernetes/

----------------------------------------------------------------------------------------------------------------------------------

Lab3.1 安装Kubernetes

课程Yaml文件下载
wget https://training.linuxfoundation.org/cm/LFS258/LFS258_V2018-02-15_SOLUTIONS.tar.bz2 --user=LFtraining --password=Penguin2014
tar -xvf ./LFS258_V2018-02-15_SOLUTIONS.tar.bz2

使用秘钥登录系统
$ ssh -i LFS458.pem student@35.226.100.87

安装Docker
$ sudo -i
# apt-get update && apt-get upgrade -y
# apt-cache madison docker.io   //查看可用的Docker版本
# apt-get install -y docker.io

<国内配置科学上网>
# mkdir -p /etc/systemd/system/docker.service.d
# cat /etc/systemd/system/docker.service.d/http-proxy.conf
[Service]
Environment="HTTP_PROXY=http://192.168.100.97:8123"
# cat /etc/systemd/system/docker.service.d/https-proxy.conf
[Service]
Environment="HTTPS_PROXY=https://192.168.100.97:8123"
# systemctl daemon-reload
# systemctl restart docker
# docker info
# systemctl show --property=Environment docker

关SWAP
# swapoff  -a
# vim /etc/fstab  (add #)

添加Kubernetes源并安装
# vim /etc/apt/sources.list.d/kubernetes.list
deb http://apt.kubernetes.io/ kubernetes-xenial main
# https_proxy=https://192.168.100.97:8123 curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
# https_proxy=https://192.168.100.97:8123 apt-get update
# apt-cache madison kubeadm
# https_proxy=https://192.168.100.97:8123 apt-get install kubeadm=1.9.6-00 kubelet=1.9.6-00 //官方手册是1.9.1-00

kubernetes初使化
# kubeadm init --pod-network-cidr 10.244.0.0/16  --apiserver-advertise-address 192.168.100.65 //cidr是Flannel中的地址段
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Your Kubernetes master has initialized successfully!
To start using your cluster, you need to run the following as a regular user:
  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config
You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
https://kubernetes.io/docs/concepts/cluster-administration/addons/
You can now join any number of machines by running the following on each node
as root:
  kubeadm join --token 50b8ee.cd69db91cbdf3177 192.168.100.65:6443 --discovery-token-ca-cert-hash sha256:b84b4143609d968215d5cb61c69b2b417a407d30c53b70f975dc787b24c56879
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
根据上面的地址 https://kubernetes.io/docs/concepts/cluster-administration/addons/ 打开后选择要配置的网络
本实验使用的网络是Flannel，打开上面的地址后找Flannel，跳到GitHub地址
https://github.com/coreos/flannel/blob/master/Documentation/kubernetes.md
向上翻一级到 Documentation 目录，找到kube-flannel.yml文件并打开，然后点Raw按钮可得到以下地址
https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
# exit

普通用户授权使用Kubectl命令
$ mkdir -p $HOME/.kube
$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
$ sudo chown $(id -u):$(id -g) $HOME/.kube/config

安装Flannel网络
$ sudo cp /root/kube-flannel.yml ./
$ kubectl apply -f kube-flannel.yml
$ ip addr 

去除master节点的Taints的NoSchedule,使master节点可以部署pod
$ kubectl get nodes
$ kubectl get pods --all-namespaces
$ kubectl describe nodes yqb-k8s-node1 | grep Taints
$ kubectl taint nodes --all  node-role.kubernetes.io/master-
$ kubectl describe nodes yqb-k8s-node1 | grep Taints

Kube命令自动补齐
$ source <(kubectl completion bash)
$ echo "source <(kubectl completion bash)" >> .bashrc
$ source <(kubeadm completion bash)
$ echo "source <(kubeadm completion bash)" >> .bashrc

----------------------------------------------------------------------------------------------------------------------------------

Lab3.2 扩展集群

安装WorkNode与Lab3.1中的部分内容一样
    1、安装Docker
    2、<国内配置科学上网>
    3、关SWAP
    4、添加Kubernetes源并安装
    
生成令牌，令牌默认保存24h
$ sudo kubeadm token list
如果没有令牌使用命令创建
$ sudo kubeadm token create
    --ttl 0 //创建永不过期的token

证书-->生成公钥-->签名
$ openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outfrom der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'
e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855

    openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt //-pubkey使用证书文件生成公钥
    open rsa -pubin -outfrom der 2>/dev/null //-pubin输出公钥，-outfroml输出格式，标准错误到null
    openssl dgst -sha256 -hex | sed 's/^.* //' //使用sha256生成和验证数字签名
    
将主机加入到集群，使用已有token或新创建的token,并把上面的加密内容放到sha256:后面
$ kubeadm join --token 50b8ee.cd69db91cbdf3177 192.168.100.65:6443 --discovery-token-ca-cert-hash sha256:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855

$ kubectl get nodes
$ kubectl get namespace
$ kubectl get pod --all-namespaces
$ ip addr //两个节点查看flannel.1的网络分别是10.244.0.0/32和10.244.1.0/32

部署应用测试及网络连通性
$ kubectl run nginx --image nginx
$ kubectl get deployments
$ kubectl describe deployment nginx
$ kubectl get events //查看集群获取和部署新应用程序所需的基本步骤
$ kubectl get deployment ng -o yaml
$ kubectl get deployment ng -o yaml > first.yaml
$ vim first.yaml //编辑并删除 creationTimestamp、resourceVersion、selfLink、uid和status下的十几行
$ kubectl create -f first.yaml
$ kubectl get deployment nginx -o yaml > second.yaml
$ diff first.yaml second.yaml
$ kubectl expose deployment/nginx //因为yaml文件中没有为Pod指定端口，所以暴露服务会报错
$ vim first.yaml //在容器的name下增加这三行
          ports:
          - containerPort: 80
            protocol: TCP
$ kubectl delete deployment nginx
$ kubectl apply -f first.yaml
$ kubectl expose deployment nginx
$ kubectl get service nginx
$ kubectl get endpoints nginx
$ kubectl describe pod nginx-7587c6fdb6-jlg5h  | grep Node //查看容器所在主机
# sudo tcpdump -i flannel.1  //在Pod所在机器抓包，包括flannel.1、cni0和veth网卡
$ curl 10.99.157.33:80 //使用群集IP访问
$ curl 10.244.1.187:80 //使用endpoint地址访问

$ kubectl get deployment nginx
$ kubectl scale deployment nginx --replicas=3 //扩展为3个pod
$ kubectl get deployment nginx
$ kubectl get endpoints nginx
$ kubectl get pod -o wide
$ kubectl delete pod nginx-7587c6fdb6-jlg5h //删除一个pod
$ kubectl get pod //发现自动重建pod,维持为三副本
$ kubectl get endpoints //发现endpoint自动更新的新的Pod地址
$ echo 111111111111111111111111 >> /usr/share/nginx/html/index.html //分别进入三个容器，往index.html文件下面分别加111、222、333
$ curl 10.99.157.33:80 //执行多次可以看到自动负载均衡

----------------------------------------------------------------------------------------------------------------------------------

Lab3.3 从集群外部访问

$ kubectl get pod
$ kubectl exec nginx-7587c6fdb6-bxj9s -- printenv | grep KUBERNETES //打印容器中的环境变量
$ kubectl get service
$ kubectl delete service nginx //删除服务
$ kubectl expose deployment nginx --type=LoadBalancer //创建一个类型为负载均衡的服务,如果类型为NodePort也可用以下方法正常访问
$ kubectl get service //查看暴露的端口为80:30063/TCP AWS和GCE有一个奇怪的地方，EXTERNAL-IP它总是显示为 <pending> 
使用浏览器访问 http://42.62.51.27:30063/
$ kubectl delete deployment nginx
$ kubectl delete endpoints nginx
$ kubectl delete service nginx

----------------------------------------------------------------------------------------------------------------------------------

Lab4.1 容器CPU和内存限制

$ kubectl run hog --image=vish/stress    //virsh/stress是压力容器
$ kubectl get deployment
$ kubectl describe deployment hog
$ kubectl get deployment hog -o yaml    //没有设置资源限制，resources: {}里是空的

$ kubectl get deployment hog -o yaml > hog.yaml
$ vim hog.yaml //删除状态等内容
        resources:   //删除{}添加下面四行
          limits:
            memory: "4Gi"
          requests:
            memory: "256Mi"
$ kubectl replace -f hog.yaml    //再次发布
$ kubectl get deployment hog -o yaml
$ kubectl run hog --image=vish/stress --limits='memory=4Gi' --requests='memory=2500Mi' //也可以用这个命令直接做资源限制
$ kubectl logs hog-ddcc66b88-wfm2q    //查看容器资源限制为0

$ vim hog.yaml //修改配置文件
        resources:
          limits:
            cpu: 1
            memory: 4Gi
          requests:
            cpu: 0.5
            memory: 500Mi
        args:    //给容器增加压力测试参数，每次分配100M间隔1s总计950M
        - -cpus
        - "2"
        - -mem-total
        - "950Mi"
        - -mem-alloc-size
        - "100Mi"
        - -mem-alloc-sleep
        - "1s"
$ kubectl replace -f hog.yaml    //再次发布
$ kubectl logs hog-67594d8556-48w65    //查看资源消耗950M内存和2个CPU线程，可以在节点上查看内存的使用

资源请求（Request）：容器所需的最小资源需求
和资源限制（Limit）：是容器不能超过的资源上限

----------------------------------------------------------------------------------------------------------------------------------

Lab 4.2 NameSpace资源限制

$ kubectl create namespace low-usage-limit
$ kubectl get namespace
$ vim low-resource-range.yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: low-resource-range
spec:
  limits:
  - default:
      cpu: 1
      memory: 1000Mi
    defaultRequest:
      cpu: 0.2
      memory: 100Mi
    type: Container
$ kubectl apply -f low-resource-range.yaml --namespace=low-usage-limit
$ kubectl get LimitRange --all-namespaces
$ kubectl run limited-hog --image=vish/stress -n low-usage-limit
$ kubectl get deployment --all-namespaces
$ kubectl get pod -n low-usage-limit
$ kubectl -n low-usage-limit get pod limited-hog-5bcbbc876b-hjfrw -o yaml    //查看Pod详情，容器资源限制将从Namespace继承

在LimitRange中，容器默认使用0.2C和100M内存，最多限制为1C和1000M内存，当容为20个的时候，有14个Running，6个Pending
此时14个容器占用总CPU和内存数达到了namespace中LimitRange的上限1C、1000M
修改了low-resource-range.yaml配置文件后，重新replace配置后，对已经存存的容器无限效，通过scale调度新增加的容器使用新的LimitRange

$ cp hog.yaml hog2.yaml
$ vim hog2.yaml
    namespace: low-usage-limit    //改默认namespace
$ kubectl apply -f hog2.yaml
查看top 

----------------------------------------------------------------------------------------------------------------------------------

Lab 4.3 更复杂的部署

一个网站的微服务部署Demo，github地址是
https://raw.githubusercontent.com/microservices-demo/microservices-demo/master/deploy/kubernetes/complete-demo.yaml

$ http_proxy=http://192.168.100.97:8123/ wget https://tinyurl.com/y8bn2awp -O complete-demo.yaml
$ cat complete-demo.yaml | grep image    //查看所需要的镜像
$ vim complete-demo.yaml    //详细查看文件，并找到namespace
$ kubectl create namespace sock-shop
$ kubectl apply -f complete-demo.yaml -n sock-shop
$ kubectl get service -n sock-shop    //查看NodePort并访问网站地址http://42.62.51.27:30001/

测试删除Pod后，Pod会自动重建,删除Deployment和Service后不会自动重建

$ kubectl delete -f complete-demo.yaml

----------------------------------------------------------------------------------------------------------------------------------

Lab 5.1 配置TLS访问

API请求必须以JSON形式传递信息。
在为您做API请求时，kubectl将.yaml转换为JSON。
API请求有很多设置，但必须包括apiVersion、kind和元数据，以及spec设置，以声明要部署的容器类型。
spec字段取决于正在创建的对象。

访问API
$ cat .kube/config | grep client-cert | awk '{print $2}' | base64 -d > client.pem
$ cat .kube/config | grep client-key | awk '{print $2}' | base64 -d > client-key.pem
$ cat .kube/config | grep  auth | awk '{print $2}' | base64 -d  > ca.pem
$ cat .kube/config | grep server

$ curl --cert client.pem --key client-key.pem --cacert ca.pem https://192.168.100.65:6443/api/v1/pods

使用json文件通过调用API创建pod
$ vim curlpod.json
{
  "kind": "Pod",
  "apiVersion": "v1",
  "metadata":{
    "name": "curlpod",
    "namespace": "default",
    "labels": {
      "name": "examplepod"
    }
  },
  "spec": {
    "containers": [{
      "name": "nginx",
      "image": "nginx",
      "ports": [{"containerPort": 80}]
    }]
  }
}

$ curl --cert client.pem --key client-key.pem --cacert ca.pem https://192.168.100.65:6443/api/v1/namespaces/default/pods -XPOST -H 'Content-Type: application/json' -d@curlpod.json
$ kubectl get pod

----------------------------------------------------------------------------------------------------------------------------------

Lab 5.2 配置TLS访问

$ kubectl get endpoints
$ strace kubectl get endpoints    //跟踪调试运行

查看所有的API
cd .kube/cache/discovery/192.168.100.65_6443/
find . | grep json

$ kubectl api-versions

$ python -m json.tool v1/serverresources.json    //以json格式查看API内容
$ python -m json.tool v1/serverresources.json | grep kind
$ python -m json.tool ./apps/v1beta1/serverresources.json | grep kind
$ python -m json.tool ./autoscaling/v1/serverresources.json | grep kind

----------------------------------------------------------------------------------------------------------------------------------

Lab 6.1 RESTful API 访问（有状态集API访问）

我们将使用curl命令以不安全的方式向集群发出API请求。
一旦我们知道了IP地址和端口，我们就可以用RESTful方式检索集群数据。
默认情况下，大多数信息是受限的，但对身份验证策略的更改可能允许更多访问。

$ kubectl config view

接下来，我们需要找到不记名令牌。
这是默认令牌的一部分。
查看令牌列表，首先在群集上，然后是默认名称空间中的令牌。
集群中的每个controllers都有一个seceret

$ kubectl get secret --all-namespaces
$ kubectl get secret 
$ kubectl describe secret default-token-vx5kg
$ kubectl describe secret default-token-vx5kg | grep token: | awk '{print $2}'
$ export token=$(kubectl describe secret default-token-vx5kg | grep token: | awk '{print $2}')

$ curl https://192.168.100.65:6443/apis --header "Authorzation : Bearer $token" -k
$ curl https://192.168.100.65:6443/api/v1 --header "Authorzation : Bearer $token" -k
$ curl https://192.168.100.65:6443/api/v1/namespaces --header "Authorzation : Bearer $token" -k    //报错
$ kubectl run -i -t busybox --image=busybox --restart=Never
# ls /var/run/secrets/kubernetes.io/serviceaccount/

----------------------------------------------------------------------------------------------------------------------------------

Lab 6.2 使用Proxy

$ kubectl proxy --api-prefix=/ &
$ curl http://127.0.0.1:8001/api
$ curl http://127.0.0.1:8001/api/v1/namespaces     //不报错

----------------------------------------------------------------------------------------------------------------------------------

Lab 6.3 使用 CronJob

每分钟创建一个容器，输出date后sleep 30s，然后关闭
$ vim cron-job.yaml

apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: date
spec:
  schedule: "*/1 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: dateperminute
            image: busybox
            args:
            - /bin/sh
            - -c
            - date; sleep 30
          restartPolicy: OnFailure

$ kubectl create -f cron-job.yaml
$ kubectl get cronjob
$ kubectl get job
$ kubectl get pod
$ kubectl get job --watch
$ kubectl get pod | grep date

查看容器日志，每个容器创建时间相差1分钟
$ kubectl logs date-1529483160-s86f9
$ kubectl logs date-1529483220-rtwhs
$ kubectl logs date-1529483280-9674n
$ kubectl logs date-1529483340-l8jz2

$ kubectl delete cronjob date

----------------------------------------------------------------------------------------------------------------------------------

Lab 7.1 使用 ReplicaSets （副本集）

$ kubectl get replicaset
$ vim rs.yaml

apiVersion: extensions/v1beta1
kind: ReplicaSet
metadata:
  name: rs-one
spec:
  replicas: 2
  template:
    metadata:
      labels:
        system: ReplicaOne
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80

$ kubectl create -f rs.yaml
$ kubectl describe replicaset rs-one
$ kubectl get pod
$ kubectl delete replicaset rs-one --cascade=false    //删除Replicaset不串联pod
$ kubectl get replicaset
$ kubectl get pod

$ kubectl create -f rs.yaml    //再次创建rs，rs会根据selector自动关联pod
$ kubectl get replicaset
$ kubectl get pods

$ kubectl edit pod rs-one-smjdj    //修改Pod的Lable，ReplicaOne改成IsolatedPod
$ kubectl get replicaset    //复本数应该还是2
$ kubectl  get pod -L system    //3个pod
$ kubectl delete replicaset rs-one    //rs和2个pod将被删除
$ kubectl delete pod -l system=IsolatedPod    //通过lable删除剩下的pod

可以删除复本集而不删除pod
创建复本集后可自动关联pod
修改pod标签后复本集自动创建新的pod的以维护副本数量
可通过标签删除pod

----------------------------------------------------------------------------------------------------------------------------------

Lab 7.2 使用 DaemonSets （守护程序集）

vim ds.yaml

apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: ds-one
spec:
  template:
    metadata:
      labels:
        system: DaemonSetOne
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80

$ kubectl create -f ds.yaml
$ kubectl get daemonset
$ kubectl describe daemonset ds-one
$ kubectl get pod -o wide
$ kubectl get pod -o wide
$ kubectl describe pod ds-one-7s5xh | grep 

与ReplicaSets相比，只是Yaml文件修改了一下Kind和Labels，删除了 replicas: 2

----------------------------------------------------------------------------------------------------------------------------------

Lab 7.3 滚动更新和回滚

$ kubectl get daemonset ds-one -o yaml    //查看默认更新策略为OnDelete
$ kubectl set image daemonset ds-one nginx=nginx:1.8.1-alpine    //使用set更新镜像
$ kubectl describe pod ds-one-7s5xh | grep Image:    //查看原来的Pod image版本没有变
$ kubectl delete pod ds-one-7s5xh
$ kubectl get pod
$ kubectl describe pod ds-one-qmsvb | grep Image:    //新增pod image版本更新了
$ kubectl rollout history daemonset ds-one    //查看更新历史列表
$ kubectl rollout history daemonset ds-one --revision=1    //对比两个历史版本的区别
$ kubectl rollout history daemonset ds-one --revision=2

$ kubectl rollout undo daemonset ds-one --to-revision=1 //回滚到版本1
$ kubectl describe pod ds-one-qmsvb | grep Image:
$ kubectl delete pod ds-one-qmsvb
$ kubectl describe pod ds-one-x9mtg  | grep Image:    //回到了原来的版本

滚动更新使用set，回滚使用 rollout undo
当updateStrategy为OnDelete时，先删除原有的Pod后，新Pod才会更新

$ kubectl get daemonset ds-one -o yaml    //查看默认更新策略为OnDelete
$ kubectl get daemonset ds-one -o yaml > ds2.yaml
$ vim ds2.yaml    //删除该删的时间和状态，修改OnDelete为RollingUpdate,修改名字为ds-two
$ kubectl create -f ds2.yaml    //重新创建ds-two
$ kubectl get pod
$ kubectl describe pod ds-two-gtv48  | grep Image:
$ kubectl edit daemonset ds-two    //使用edit更新image为nginx:1.8.1-alpine
$ kubectl get daemonset ds-two
$ kubectl get pod
$ kubectl describe pod ds-two-q7jlp | grep Image:
$ kubectl rollout status daemonset ds-two    //查看循环更新状态
$ kubectl rollout history daemonset ds-two
$ kubectl rollout history daemonset ds-two --revision=2
$ kubectl delete daemonset ds-two

当updateStrategy为RollingUpdate时，Pod会自动循环更新（删一个建一个）

----------------------------------------------------------------------------------------------------------------------------------

Lab 8.1 Service

服务（也称为微服务）是声明访问Pod的逻辑集合的策略的对象。

$ vim nginx-one.yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx-one
  labels:
    system: secondOne
  namespace: accounting
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - image: nginx:1.7.9
        imagePullPolicy: Always
        name: nginx
        ports:
        - containerPort: 8080
          protocol: TCP
      nodeSelector:
        system: secondOne

$ kubectl get node --show-labels
$ kubectl create namespace accounting
$ kubectl -n accounting get deployment
$ kubectl -n accounting get pod    //状态是Pending
$ kubectl -n accounting describe pod    //看到报错，没有可用节点

$ kubectl label node yqb-k8s-node2 system=secondOne    //为节点添加Lable后，容器启动成功
$ kubectl get nodes --show-labels
$ kubectl get pods -n accounting

$ kubectl -n accounting get pod --show-labels    //查看容器的Lable
$ kubectl get pod --all-namespaces -l app=nginx    //通过Lable筛选容器

$ kubectl get service -n accounting
$ kubectl -n accounting expose deployment nginx-one    //服务暴露
$ kubectl get service -n accounting
$ kubectl get endpoints -n accounting

虽然暴露的是8080端口，但是应用未侦听此端口，所以访问endpoints的8080端口失败，访问80端口成功

$ kubectl delete deployment nginx-one -n accounting    //删除Deployment后，Service和Endpoints并不会被删除
$ vim nginx-one.yaml    //8080改成80
$ kubectl create -f nginx-one.yaml    //重新部署
kubectl -n accounting delete service nginx-one    //删除Service
kubectl -n accounting expose deployment nginx-one    //服务暴露，默认为ClusterIP
kubectl get endpoints -n accounting
kubectl get service -n accounting

kubectl -n accounting delete service nginx-one    //删除服务暴露，为后面的实验做准备

----------------------------------------------------------------------------------------------------------------------------------

Lab 8.2 NodePort

$ kubectl expose deployment -n accounting nginx-one --type=NodePort --name=service-lab    //接上一章的Deployment，暴露服务为NodePort，启名为service-lab
$ kubectl get service -n accounting
$ kubectl describe service -n accounting    //查看自动生成的端口30567

$ curl 192.168.100.65:30567
http://42.62.51.27:30567/    //使用外部网络，访问这个地址也可以访问到

----------------------------------------------------------------------------------------------------------------------------------

Lab 8.3 使用Lables管理资源

$ kubectl delete pod -n accounting -l app=nginx    //通过lable删除pod
$ kubectl get pod -n accounting    //deployment自动部署新的pod
$ kubectl get deployment -n accounting --show-labels    //查看deployment的lable
$ kubectl delete deployment -n accounting -l system=secondOne    //通过lable删除deployment

$ kubectl get node --show-labels
$ kubectl label node yqb-k8s-node2 system-    //从节点上删除lable

----------------------------------------------------------------------------------------------------------------------------------

Lab 9.1 ConfigMap

#ConfigMap可以使用的创建方式包括文本、文件、目标和Yaml文件中
#可以单独调用configmap中的某一个key-value给容器，也可以将configmap中的所有key-value给容器

#创建文本、文件和目录形式的key-value
$ mkdir primary
$ echo c > primary/cyan
$ echo m > primary/magenta
$ echo y > primary/yellow
$ echo k > primary/black
$ echo "known as key" >> primary/black
$ echo blue > favorite

#使用文本、文件和目录形式的key-value创建configmap
$ kubectl create configmap colors --from-literal=text=black --from-file=./favorite --from-file=./primary/
    --from-literal=text=black    //文本形式
    --from-file=./favorite    //文件形式
    --from-file=./primary/    //目录形式
$ kubectl get configmap colors
$ kubectl get configmap colors -o yaml

#在创建Pod时使用configmap把参数传递到容器
$ vim simpleshell.yaml
apiVersion: v1
kind: Pod
metadata:
  name: shell-demo
spec:
  containers:
  - name: nginx
    image: nginx
    env:
    - name: ilike
      valueFrom:
        configMapKeyRef:
          name: colors
          key: favorite

$ kubectl create -f simpleshell.yaml

#验证效果，看到blue
$ kubectl exec -ti shell-demo -- /bin/bash -c 'echo $ilike'

#使用envFrom将configmap中的所有key-value放入环境变量中
$ vim simpleshell.yaml
apiVersion: v1
kind: Pod
metadata:
  name: shell-demo
spec:
  containers:
  - name: nginx
    image: nginx
#    env:
#    - name: ilike
#      valueFrom:
#        configMapKeyRef:
#          name: colors
#          key: favorite
    envFrom:
    - configMapRef:
        name: colors

#下面命令可以看到configmap colors中的所有key-value
$ kubectl exec -ti shell-demo -- /bin/bash -c 'env'
$ kubectl delete pod shell-demo

#使用yaml文件创建configmap
$ vim car-map.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: fast-car
  namespace: default
data:
  car.make: Ford
  car.model: Mustang
  car.trim: Shelby

$ kubectl get configmap fast-car -o yaml

#将configmap做为卷，挂载到pod中
$ vim simpleshell.yaml
apiVersion: v1
kind: Pod
metadata:
  name: shell-demo
spec:
  containers:
  - name: nginx
    image: nginx
    volumeMounts:
    - name: car-vol
      mountPath: /etc/cars
  volumes:
    - name: car-vol
      configMap:
        name: fast-car

$ kubectl create -f simpleshell.yaml
$ kubectl exec -ti shell-demo -- /bin/bash -c 'df -Th'
$ kubectl exec -ti shell-demo -- /bin/bash -c 'cat /etc/cars/car.trim'
#可以看到输出内容为Shelby，即  car.trim: Shelby 的Key-value对应关系

$ kubectl delete pod shell-demo
$ kubectl delete configmap fast-car colors

----------------------------------------------------------------------------------------------------------------------------------

Lab 9.2 创建 NFS 类型的 PV

#安装并配置NFS
$ sudo apt-get install nfs-kernel-server
$ sudo mkdir /opt/sfw/
$ sudo chmod 1777 /opt/sfw/
$ sudo bash -c  'echo software > /opt/sfw/hello.txt'
$ sudo vim /etc/exports
/opt/sfw/ *(rw,sync,no_root_squash,subtree_check)
$ sudo exportfs -ra    //重新加载配置文件
$ sudo apt-get install nfs-common
$ sudo showmount  -e

node2:~$ sudo showmount -e 192.168.100.65
node2:~$ sudo mount 192.168.100.65:/opt/sfw /mnt
node2:~$ ls /mnt/
node2:~$ cat /mnt/hello.txt

#创建PV的yaml文件
$ vim PVol.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pvvol-1
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  nfs:
    path: /opt/sfw
    server: 192.168.100.65
    readOnly: false
    
#使用yaml文件创建PV
$ kubectl create -f PVol.yaml
$ kubectl get pv

----------------------------------------------------------------------------------------------------------------------------------

Lab 9.3 创建 PVC

$ kubectl get pvc
$ vim pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-one
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 200Mi

$ kubectl create -f pvc.yaml
$ kubectl get pvc

#STATUS由Available变成了Bound    
$ kubect get pv

#创建POD并使用PVC挂载卷
$ vim nfs-pod.yaml
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
  generation: 1
  labels:
    run: nginx
  name: nginx-nfs
  resourceVersion: "1411"
spec:
  replicas: 1
  selector:
    matchLabels:
      run: nginx
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        run: nginx
    spec:
      containers:
      - image: nginx
        imagePullPolicy: Always
        name: nginx
        volumeMounts:
        - name: nfs-vol
          mountPath: /opt
        ports:
        - containerPort: 80
          protocol: TCP
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      volumes: 
      - name: nfs-vol
        persistentVolumeClaim:
          claimName: pvc-one
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30

#主要是这四行
      volumes: 
      - name: nfs-vol   //卷名称
        persistentVolumeClaim:
          claimName: pvc-one    //调用的PVC

$ kubectl create -f nfs-pod.yaml
$ kubectl get pod
#查看 Mounts: 和 Volumes: 部分
$ kubectl describe pod nginx-nfs-7f7b7459b9-pvf9r
$ kubectl exec -ti nginx-nfs-7f7b7459b9-pvf9r -- /bin/bash -c 'cat /opt/hello.txt'
$ kubectl exec -ti nginx-nfs-7f7b7459b9-pvf9r -- /bin/bash -c 'df -Th'
192.168.100.65:/opt/sfw        nfs4    41G  6.8G   32G  18% /opt

$ kubectl delete deployment nginx-nfs
$ kubectl delete pvc pvc-one
$ kubectl delete pv pvvol-1

#PV是持久化卷，需要定义卷的来源，例如NFS服务器地址和目录
#PVC是持久化卷申请的声明或叫做请求
#当只有一个PV和PVC时，在yaml文件中并未指明关系，PVC会自动使用PV的卷，PV会自动绑定到PVC
#在9.2和9.3中，容器的卷名称是“nfs-vol”，容器使用使用PVC“pvc-one”，自动查到PV并使用
#使用PVC可以为用户隐藏起来PV的IP、目录等配置参数，简化用户操作。

----------------------------------------------------------------------------------------------------------------------------------

Lab 9.4 使用 ResourceQuota 限制 PVC 数量和使用

#创建资源配额yaml文件，限制在本namespace中pvc为10个，存储为500M
$ vim storage-quota.yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: storagequota
spec:
  hard:
    persistentvolumeclaims: "10"
    requests.storage: "500Mi"

#创建namespace并查看详情，默认Quota和Limits都为空（No resource quota.  No resource limits.）
$ kubectl create namespace small
$ kubectl describe namespace small

#使用9.2和9.3的yaml文件small名字空间中创建pv和pvc
$ kubectl create -f PVol.yaml -n small
$ kubectl create -f pvc.yaml -n small

#使用yaml文件创建资源配额
$ kubectl create -f storage-quota.yaml -n small

#再次查看namespace的详情，Quota有内容了
$ kubectl describe namespace small

#在small中创建deployment并确保Mount:已经挂载
$ kubectl create -f nfs-pod.yaml -n small
$ kubectl get deployment -n small nginx-nfs
$ kubectl describe deployment -n small nginx-nfs
$ kubectl get pod -n small nginx-nfs-7f7b7459b9-l5znt
$ kubectl describe pod -n small nginx-nfs-7f7b7459b9-l5znt
$ kubectl describe namespace small

#在NFS目录中创建一个300M的文件，查看配额限制无变化（NFS共享目录的大小不计入配额限制）
$ sudo dd if=/dev/zero of=/opt/sfw/bigfile bs=1M count=300
$ kubectl describe namespace small

#Pod删除后配额没有变化
$ kubectl get deployment -n small
$ kubectl delete deployment -n small nginx-nfs
$ kubectl describe namespace small

#删除PVC后PV的状态变成了Released
$ kubectl get pv -n small
$ kubectl get pvc -n small
$ kubectl delete pvc -n small pvc-one
$ kubectl get pv -n small
$ kubectl get pv -n small pvvol-1 -o yaml    #默认策略 persistentVolumeReclaimPolicy: Retain
$ kubectl delete pv pvvol-1

#上面部署把东西都删除了，重建PV并用patch修改Retain为Delete
$ kubectl create -f PVol.yaml
$ kubectl get pv -n small pvvol-1 -o yaml 
$ kubectl patch pv -n small pvvol-1 -p '{"spec":{"persistentVolumeReclaimPolicy":"Delete"}}'
$ kubectl get pv -n small    //清理策略变成了Delete
$ kubectl describe namespace small    //当前的配额使用量为0，因为还没有PVC
$ kubectl create  -f pvc.yaml -n small
$ kubectl describe namespace small    //创建PVC后，使用变成了200M

$ kubectl get resourcequota -n small
$ kubectl delete resourcequota -n small storagequota    //删除配额配置
$ vim storage-quota.yaml    //把配额限制改成100M
$ kubectl create -f storage-quota.yaml -n small    //重新创建配额限制
$ kubectl describe namespace small    //配额超过了硬限制
$ kubectl create -f nfs-pod.yaml -n small    //重新创建Pod，没有报错。
$ kubectl describe deployment -n small nginx-nfs
$ kubectl get pod -n small    //pod确实在运行

$ kubectl delete deployment -n small nginx-nfs
$ kubectl delete pvc -n small pvc-one
$ kubectl get pv -n small    //删除PVC后PV的状态变成了Failed（试图在删除，但是失败了）
$ kubectl describe pv -n small pvvol-1    //这里可以看到因为缺少插件（ Error getting deleter volume plugin for volume "pvvol-1": no deletable volume plugin matched），NFS存在这种问题
$ kubectl delete pv -n small pvvol-1    //手动删除一下。现在又全部删除了。

$ kubectl create -f low-resource-range.yaml -n small    //创建CPU和内存限制
$ kubectl describe namespace small
$ vim PVol.yaml    //修改persistentVolumeReclaimPolicy: Recycle
$ kubectl create -f PVol.yaml -n small    //重新创建PV
$ kubectl get pv -n small    //PV的清理策略为Recycle
$ kubectl create -f pvc.yaml -n small    //创建PVC发现报错（只有当Resource Limits生效时，Resource Quotas才会生效 ）
$ kubectl edit resourcequota -n small    //编辑配额限制为500Mi
$ kubectl create -f pvc.yaml -n small    //再次创建PVC成功
$ kubectl create -f nfs-pod.yaml -n small
$ kubectl delete deployment -n small nginx-nfs
$ kubectl get pvc -n small
$ kubectl get pv -n small
$ kubectl delete pvc -n small pvc-one
$ kubectl get pv -n small    //删除PVC后，PV的状态自动回收，变成Available
$ kubectl delete pv -n small pvvol-1
$ kubectl delete namespace small

#PV和PVC介绍
    PV：PersistentVolume 全局资源
    PVC：PersistentVolumeClaim NameSpace资源

PV的提供形式为两种：
    Static：在集群里手动创建PV，供PVC使用
    Dynamic：当集群里没有PV符合PVC请求时，集群里有StorageClass资源 ，且PVC里StorageClass资源的描述，PVC会自动通过StorageClass动态创建PV

PV和PVC生命周期：
    Binding：创建PVC时，PVController查找最合适的PV并建立绑定关系，一个PV只能绑定给一个PVC
    Using：Pod把PVC当做Volume使用
    Releasing：删除PVC后，PV状态变成Released，并准备被回收
    Reclaiming：PV回收策略，有三种
        Retained：PV保持原有数据并允许用户手动回收数据
        Recycled：删除数据，并允许PV被绑定到其它PVC
        Deleted：删除数据并删除PV

PV的四状态:
    Available：PV可以被使用
    Bound：PV被绑定到了PVC
    Released：被绑定的PVC被删除，可以被Reclaim
    Failed：自动回收失败

PVC的三种状：
    Pending：等待可用的PV
    Bound：PVC绑定了PV
    Lost：找不到可绑定的PV

PV三种访问模式：
    ReadWriteOnce：单节点读写
    ReadOnlyMany：多节点只读
    ReadWriteMany：多节点读写

----------------------------------------------------------------------------------------------------------------------------------

Lab 11.1 使用 Labels 分配Pods

#先删除已有的deployment，获取每个node上的容器数量，然后继续
#给两个节点找不同的标签，分别是vip和other
$ kubectl describe node | grep -i label
$ kubectl describe node | grep -i taint
$ kubectl label node yqb-k8s-node1 status=vip
$ kubectl label node yqb-k8s-node2 status=other
$ kubectl get node --show-labels

#Pod在有vip标签的主机上
$ vim vip.yaml
apiVersion: v1
kind: Pod
metadata:
  name: vip
spec:
  containers:
  - name: vip1
    image: busybox
    args:
    - sleep
    - "1000000"
  - name: vip2
    image: busybox
    args:
    - sleep
    - "1000000"
  - name: vip3
    image: busybox
    args:
    - sleep
    - "1000000"
  - name: vip4
    image: busybox
    args:
    - sleep
    - "1000000"
  nodeSelector:
    status: vip

$ kubectl create -f vip.yaml
$ kubectl  get pod -o wide

#pod在另一台主机上
$ kubectl delete pod vip
$ vim vip.yaml
#  nodeSelector:
#    status: vip
$ kubectl create -f vip.yaml
$ kubectl  get pod -o wide

#同样的方法部署other
$ cp vip.yaml other.yaml
$ sed  -i s/vip/other/g other.yaml
$ kubectl create -f other.yaml
$ kubectl get pod -o wide

$ kubectl delete pod vip other

----------------------------------------------------------------------------------------------------------------------------------

Lab 11.2 使用 Taints 控制 Pod 部署

#默认部署应用分布在多个节点上
$ vim taint.yaml
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: taint-deployment
spec:
  replicas: 8
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80

$ kubectl apply -f taint.yaml
$ kubectl get pod -o wide
$ kubectl delete deployment taint-deployment

#Tains有三种类型 NoSchedule、PreferNoSchedule 和 NoExecute
#NoExecute可以使用正在运行的容器移动到其它节点上

#第2个节点添加taint再次部署应用，这时第2节点上还是会有pod，但可能会比第1次部署时候少。
$ kubectl taint node yqb-k8s-node2 bubba=value:PreferNoSchedule
$ kubectl describe node | grep -i taint
$ kubectl apply -f taint.yaml
$ kubectl get pod -o wide

$ kubectl delete deployment taint-deployment
$ kubectl taint node yqb-k8s-node2 bubba-

#在第2个节点上把taint设置成NoSchedule，第2个节点将没有Pod被部署
$ kubectl taint node yqb-k8s-node2 bubba=value:NoSchedule
$ kubectl apply -f taint.yaml
$ kubectl get pod -o wide

$ kubectl delete deployment taint-deployment
$ kubectl taint node yqb-k8s-node2 bubba-

#先部署然后设置taint为NoExecute
#配置了taint的node上的所有容器将被迁移走
#删除了taint后，节点运行所必需的容器迁移回原节点，用户部署的不一定迁
$ kubectl apply -f taint.yaml
$ kubectl  get pod -o wide
$ kubectl taint node yqb-k8s-node2 bubba=value:NoExecute
$ kubectl  get pod -o wide
$ kubectl taint node yqb-k8s-node2 bubba-
$ kubectl  get pod -o wide
$ kubectl delete deployment taint-deployment

$ kubectl apply -f taint.yaml
$ kubectl  get pod -o wide
$ kubectl delete deployment taint-deployment


#将node2设置为drain状态，部署应用后node2上不会创建pod，但node2的基础容器会正常运行
$ kubectl get node
$ kubectl drain yqb-k8s-node2    //默认会有一个报错无影响
$ kubectl get node
$ kubectl apply -f taint.yaml
$ kubectl get pod -o wide

$ kubectl delete deployment taint-deployment

#删除node2的drain状态，重新部署，一切恢复默认状态
$ kubectl get node
$ kubectl uncordon yqb-k8s-node2
$ kubectl get node
$ kubectl apply -f taint.yaml
$ kubectl get pod -o wide
$ kubectl delete deployment taint-deployment

----------------------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------------------------

##学习扩展##

Kubernetes支持暴露端服务的四种类型
    $ kubectl expose deployment nginx --type=xxx
    "ClusterIP", "ExternalName", "LoadBalancer", "NodePort"

ClusterIP：
    默认只允许集群内部访问
    外部访问时可以通过开启Kubernetes proxy 模式，使用API地址来访问服务
        $ kubectl proxy --port=8080
        http://localhost:8080/api/v1/proxy/namespaces/<NAMESPACE>/services/<SERVICE-NAME>:<PORT-NAME>/ 

NodePort：
    自动创建ClusterIP
    在所有节点上开放一个特定端口，任何发送到该端口的流量都被转发到ClusterIP
    端口范围只能是 30000-32767
    如果节点/VM 的 IP 地址发生变化，用户访问时需要输入新的地址


LoadBalancer：
    自动创建ClusterIP
    自动创建NodePort
    依赖于IaaS服务
    IaaS提供一个单独的 IP 地址，转发所有流量到你的服务

ExternalName：
    使用CNAME记录,将服务映射到外部

删除namespace后，namespace下的所有资源将被删除

----------------------------------------------------------------------------------------------------------------------------------

###排错###

环境描述：
node1 双网卡
node2 单网卡

问题描述：
跨节点无法访问Cluster IP
node1 和 node2 之间的10.244.0.0段的地址无法通信

解决过程：
kubectl -n kube-system logs -f kube-flannel-ds-9n9ww kube-flannel   //查看flannel日志

1 main.go:475] Determining IP address of default interface
1 main.go:488] Using interface with name ens3 and address 192.168.200.227
1 main.go:505] Defaulting external address to interface address (192.168.200.227)
1 kube.go:131] Waiting 10m0s for node controller to sync
1 kube.go:294] Starting kube subnet manager
1 kube.go:138] Node controller sync successful
1 main.go:235] Created subnet manager: Kubernetes Subnet Manager - yqb-k8s-node1
1 main.go:238] Installing signal handlers
1 main.go:353] Found network config - Backend type: vxlan

发现选择了ens3为flannel的网卡，应该使用ens4
kubectl delete -f kube-flannel.yml
reboot
vim kube-flannel.yml
        - /opt/bin/flanneld
        - --iface-regex=192.168.100.*    //增加这一行
        - --iface=ens3    //如果每台机器的网卡名都一样，可以使用这一行
kubectl apply -f  kube-flannel.yml

参考：
https://github.com/coreos/flannel/issues/981
https://github.com/coreos/flannel/issues/967

